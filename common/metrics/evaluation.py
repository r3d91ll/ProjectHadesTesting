"""
Evaluation Metrics for RAG Experiments

This module provides utility functions for calculating and recording various
evaluation metrics for RAG systems across all experimental phases.
"""

import numpy as np
from typing import Dict, List, Any, Union, Optional
import json
import os
from datetime import datetime


class MetricsCalculator:
    """Class for calculating and aggregating metrics for RAG experiments."""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the metrics calculator.
        
        Args:
            config: Configuration dictionary with metrics settings
        """
        self.config = config
        self.metrics = config.get("metrics", [])
        self.results_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
            "data", "results"
        )
        os.makedirs(self.results_dir, exist_ok=True)
    
    def calculate_retrieval_metrics(
        self, 
        relevant_docs: List[str],
        retrieved_docs: List[str],
        k_values: List[int] = [1, 3, 5]
    ) -> Dict[str, float]:
        """
        Calculate precision, recall, and other retrieval metrics.
        
        Args:
            relevant_docs: List of relevant document IDs (ground truth)
            retrieved_docs: List of retrieved document IDs
            k_values: Values of k for which to calculate precision@k and recall@k
            
        Returns:
            Dictionary of metric names and values
        """
        results = {}
        
        # Convert to sets for easier computation
        relevant_set = set(relevant_docs)
        
        # Calculate metrics for each k value
        for k in k_values:
            if k <= len(retrieved_docs):
                retrieved_at_k = set(retrieved_docs[:k])
                
                # Precision@k
                if len(retrieved_at_k) > 0:
                    precision = len(relevant_set.intersection(retrieved_at_k)) / len(retrieved_at_k)
                else:
                    precision = 0.0
                results[f"precision@{k}"] = precision
                
                # Recall@k
                if len(relevant_set) > 0:
                    recall = len(relevant_set.intersection(retrieved_at_k)) / len(relevant_set)
                else:
                    recall = 0.0
                results[f"recall@{k}"] = recall
        
        # Mean Reciprocal Rank (MRR)
        mrr = 0.0
        for i, doc_id in enumerate(retrieved_docs):
            if doc_id in relevant_set:
                mrr = 1.0 / (i + 1)
                break
        results["mrr"] = mrr
        
        # Calculate nDCG
        if "ndcg" in self.metrics and len(relevant_set) > 0:
            # Create relevance array (1 if document is relevant, 0 otherwise)
            relevance = [1 if doc in relevant_set else 0 for doc in retrieved_docs]
            results["ndcg"] = self._calculate_ndcg(relevance)
        
        return results
    
    def _calculate_ndcg(self, relevance: List[int], k: Optional[int] = None) -> float:
        """
        Calculate Normalized Discounted Cumulative Gain.
        
        Args:
            relevance: List of relevance scores (1 for relevant, 0 for not relevant)
            k: Cut-off for calculating nDCG@k (defaults to length of relevance list)
            
        Returns:
            nDCG value
        """
        if k is None:
            k = len(relevance)
        else:
            k = min(k, len(relevance))
        
        # Handle empty or all zeros case
        if sum(relevance[:k]) == 0:
            return 0.0
        
        # Calculate DCG
        dcg = 0.0
        for i in range(min(k, len(relevance))):
            # Using log base 2 as is standard for nDCG
            dcg += relevance[i] / np.log2(i + 2)  # +2 because log_2(1) = 0
        
        # Calculate ideal DCG (reordering relevance to be in descending order)
        ideal_relevance = sorted(relevance, reverse=True)
        idcg = 0.0
        for i in range(min(k, len(ideal_relevance))):
            idcg += ideal_relevance[i] / np.log2(i + 2)
        
        # Calculate nDCG
        if idcg > 0:
            ndcg = dcg / idcg
        else:
            ndcg = 0.0
            
        return ndcg
    
    def calculate_answer_quality_metrics(
        self, 
        reference_answer: str,
        generated_answer: str
    ) -> Dict[str, float]:
        """
        Calculate metrics for answer quality (ROUGE, BLEU, etc.)
        
        Note: This function is a placeholder. Actual implementation would use
        libraries like rouge, sacrebleu, and bert-score.
        
        Args:
            reference_answer: The reference/ground truth answer
            generated_answer: The answer generated by the RAG system
            
        Returns:
            Dictionary of metric names and values
        """
        results = {}
        
        # Placeholder for ROUGE score calculation
        # In a real implementation, use the rouge library
        results["rouge_l"] = 0.0
        
        # Placeholder for BLEU score calculation
        # In a real implementation, use the sacrebleu library
        results["bleu"] = 0.0
        
        # Placeholder for BERTScore calculation
        # In a real implementation, use the bert_score library
        results["bertscore"] = 0.0
        
        return results
    
    def save_results(
        self, 
        phase: str,
        implementation: str,
        model: str,
        dataset: str,
        results: Dict[str, Any]
    ) -> None:
        """
        Save experiment results to a JSON file.
        
        Args:
            phase: Experiment phase (phase1, phase2, phase3)
            implementation: Implementation name (pathrag, graphrag, etc.)
            model: Model name (original, qwen25, xnx)
            dataset: Dataset name
            results: Dictionary of results to save
        """
        # Create timestamp for unique filenames
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Create filename
        filename = f"{phase}_{implementation}_{model}_{dataset}_{timestamp}.json"
        filepath = os.path.join(self.results_dir, filename)
        
        # Add metadata
        results_with_meta = {
            "phase": phase,
            "implementation": implementation,
            "model": model,
            "dataset": dataset,
            "timestamp": timestamp,
            "results": results
        }
        
        # Save to file
        with open(filepath, 'w') as f:
            json.dump(results_with_meta, f, indent=2)
        
        print(f"Results saved to {filepath}")
        
        return filepath
